# Mosaic-Knot Merge

This small README explains the merged dataset created by `merge_knot_data.py` and how to regenerate or inspect it.

## Purpose

The repository contains a scraper and merge script that align knot metadata (`knotinfo.csv`) with mosaic images downloaded into `mosaics/`.
The main derived artifact is `mosaics/merged_knotinfo.csv`, a left-join of `knotinfo.csv` with the mosaic index. The merged file includes the parsed mosaic grid size and the matched image filename/url.

## Key files

- `knotinfo.csv` — source knot metadata (original). Contains a column `Mosaic/Tile-Number` with values like `{ 7 ; 27 }`.
- `mosaics/index.csv` — index of downloaded mosaic images (columns: `name`, `filename`, `url`).
- `mosaics/merged_knotinfo.csv` — merged output (generated by `merge_knot_data.py`).
- `merge_knot_data.py` — script that performs matching and writes the merged CSV.

## Columns in `mosaics/merged_knotinfo.csv`

The output uses snake_case column names. Important columns:

- `Name` — knot name from `knotinfo.csv` (e.g. `10_1`).
- `crossing_number` — original `Crossing Number`.
- `jones_polynomial` — original `Jones` value.
- `hyperbolic_volume` — original `Volume` value.
- `meridian_length` — original `Meridian Length` value.
- `mosaic_num` — parsed mosaic number (from `Mosaic/Tile-Number` or inferred from matched filename).
- `tile_num` — parsed tile number (from `Mosaic/Tile-Number` or inferred from matched filename).
- `image_filename_matched` — filename of the matched mosaic image (from `mosaics/`).
- `image_url_matched` — original URL of the matched image.

Notes
- The original `Mosaic/Tile-Number` column is intentionally removed from the merged CSV because `mosaic_num` and `tile_num` are provided separately.
- Matching strategy tries (in order): exact filename, normalized ID (e.g. `10_001` ↔ `10_1`), normalized name exact, and a fallback fuzzy name match.

## How to regenerate `mosaics/merged_knotinfo.csv`

Activate your virtual environment (PowerShell example) and run the script:

```powershell
# activate the venv (adjust path if your venv is elsewhere)
.\knotenv\Scripts\Activate.ps1

# run the merge script
python merge_knot_data.py
```

After running you should see a summary printed and `mosaics/merged_knotinfo.csv` overwritten with the latest merged table.

## Quick checks

- Show the first 10 lines (PowerShell):

```powershell
Get-Content mosaics\merged_knotinfo.csv -TotalCount 10
```

- Inspect a row with pandas (Python REPL):

```python
import pandas as pd
df = pd.read_csv('mosaics/merged_knotinfo.csv')
print(df.head())
```

## Next suggestions

- Convert `mosaic_num` and `tile_num` to integers (if you want strict types) — I can update the script to coerce/format them.
- Add a `mosaic_id_norm` column (canonical id like `10_1`) if you prefer to store a canonical join key.
- Improve fuzzy matching using `rapidfuzz` for better similarity scores.

If you'd like any of the follow-ups implemented (coerce types, add canonical id, or stronger fuzzy matching), tell me which and I'll apply it and run the script for you.

## Tile-prob dataset and experiments

This repository also contains code and artifacts for training models that map tile-prob matrices (numpy .npy files) to knot names. The additional pipeline stages and scripts include:

- `tile_matrices/` — source tile matrix images used to build tile-prob arrays.
- `tile_probs_from_matrices/` — per-knot folders containing `tile_probs.npy` (these are the model inputs).
- `create_rotated_tile_probs.py` — create rotated variants (suffixes `_rot90`, `_rot180`, `_rot270`) of existing `tile_probs.npy` folders to expand the training pool.
- `dataset_tile_probs.py` — dataset loader for the tile-prob arrays.
- `model_tileprob_cnn.py` — CNN feature extractor used by both supervised and prototypical models.
- `train_cnn.py` — supervised training script (train a classifier on rotated variants, validate on originals).
- `train_prototypical.py` — episodic prototypical trainer (N-way K-shot training with on-the-fly augmentation).

## Evaluation and plotting

- `scripts/parse_proto_logs_and_plot.py` — parse training logs and generate CSVs and comparison plots (e.g., `plots/compare_top1.png`, `plots/compare_top3_3shot.png`). Note: PowerShell redirected logs can be UTF-16; the parser detects BOMs / null bytes.
% (Confusion-matrix evaluation scripts removed from public quick-start.)

## Quick start (reproduce the prototypical experiments)

1. Activate your virtualenv (example for this repo):

```powershell
.\knotenv\Scripts\Activate.ps1
```

2. (Optional) Generate rotated training variants if not already present:

```powershell
python create_rotated_tile_probs.py
```

3. Run a prototypical experiment (example: 20-way 5-shot):

```powershell
python train_prototypical.py --epochs 10 --episodes-per-epoch 200 --N 20 --K 5 --Q 5 --eval --save-every 1 --save-dir checkpoints/proto_run
```

4. Parse logs and create comparison plots:

```powershell
python scripts/parse_proto_logs_and_plot.py
```

5. The report is in `reports/prototypical_experiment_report.tex` and plots are in `plots/`.

## Outputs and locations

- `logs/` — training logs for prototypical runs (e.g., `logs/proto_10way.log`).
- `checkpoints/` — saved model checkpoints per experiment (e.g., `checkpoints/proto_run/proto_epoch_10.pt`).
	- `plots/` — CSVs and PNGs produced by the parser and evaluation scripts (comparison plots, per-class CSVs).
- `reports/` — LaTeX report source and (after compiling) the PDF.

## Troubleshooting

- If plots are blank after parsing logs, check `logs/` files — PowerShell redirection may produce UTF-16 output; run the parser with the original console output or ensure `logs/` are in UTF-8.
	- If any evaluation script fails with `ModuleNotFoundError`, run it from the repository root or ensure the repo root is on `PYTHONPATH`.
- For LaTeX compilation issues, open `reports/prototypical_experiment_report.tex` and ensure the `\includegraphics` paths point to files under `plots/` (report compiles into the `reports/` directory so image paths are typically `../plots/...`).

If you want, I can further expand the README with example outputs (embed the comparison PNGs) or add a minimal `requirements.txt` snippet for the experiment environment.
